{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a6a065",
   "metadata": {},
   "source": [
    "# Agentic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061af7f0",
   "metadata": {},
   "source": [
    "Here we give some code for a simple AI agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc5c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Imports\n",
    "import random\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89813645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ² Random Environment Generation Demo\n",
      "==================================================\n",
      "\n",
      "ðŸŒ Random Environment #1:\n",
      "ðŸŒ Generated 5 items randomly on 6x6 grid\n",
      "ðŸ“¦ Item locations: {(1, 5): 'treasure', (1, 4): 'coin', (4, 4): 'key', (1, 1): 'tool', (3, 3): 'potion'}\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "ðŸ¤– â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ ðŸ”§ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ ðŸ§ª â¬œ â¬œ \n",
      "â¬œ ðŸª™ â¬œ â¬œ ðŸ—ï¸  â¬œ \n",
      "â¬œ ðŸ’° â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 5, Collected: 0\n",
      "\n",
      "ðŸ“Š Average distance from start: 5.4\n",
      "ðŸ“¦ Item types: ['treasure', 'coin', 'key', 'tool', 'potion']\n",
      "\n",
      "ðŸŒ Random Environment #2:\n",
      "ðŸŒ Generated 5 items randomly on 6x6 grid\n",
      "ðŸ“¦ Item locations: {(2, 3): 'food', (3, 0): 'potion', (1, 5): 'tool', (2, 2): 'treasure', (3, 1): 'gem'}\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "ðŸ¤– â¬œ â¬œ ðŸ§ª â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ ðŸ’Ž â¬œ â¬œ \n",
      "â¬œ â¬œ ðŸ’° â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ ðŸŽ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ ðŸ”§ â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 5, Collected: 0\n",
      "\n",
      "ðŸ“Š Average distance from start: 4.4\n",
      "ðŸ“¦ Item types: ['food', 'potion', 'tool', 'treasure', 'gem']\n",
      "\n",
      "ðŸŒ Random Environment #3:\n",
      "ðŸŒ Generated 5 items randomly on 6x6 grid\n",
      "ðŸ“¦ Item locations: {(1, 3): 'scroll', (3, 2): 'tool', (0, 2): 'potion', (1, 0): 'coin', (4, 3): 'food'}\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "ðŸ¤– ðŸª™ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "ðŸ§ª â¬œ â¬œ ðŸ”§ â¬œ â¬œ \n",
      "â¬œ ðŸ“œ â¬œ â¬œ ðŸŽ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 5, Collected: 0\n",
      "\n",
      "ðŸ“Š Average distance from start: 3.8\n",
      "ðŸ“¦ Item types: ['scroll', 'tool', 'potion', 'coin', 'food']\n",
      "\n",
      "ðŸ”¬ Comparing Agent Performance on Random vs Fixed Environments:\n",
      "\n",
      "ðŸŽ¯ Agent on Random Environment:\n",
      "ðŸŒ Generated 4 items randomly on 5x5 grid\n",
      "ðŸ“¦ Item locations: {(0, 3): 'food', (2, 1): 'tool', (4, 0): 'key', (0, 2): 'coin'}\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "ðŸ¤– â¬œ â¬œ â¬œ ðŸ—ï¸  \n",
      "â¬œ â¬œ ðŸ”§ â¬œ â¬œ \n",
      "ðŸª™ â¬œ â¬œ â¬œ â¬œ \n",
      "ðŸŽ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 4, Collected: 0\n",
      "\n",
      "ðŸ§  Decided: explore (prob-based)\n",
      "ðŸ¤– Agent action: move east\n",
      "ðŸ“ Agent observed: Moved east from [0, 0] to [1, 0] (Energy: 0.95)\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "â¬œ ðŸ¤– â¬œ â¬œ ðŸ—ï¸  \n",
      "â¬œ â¬œ ðŸ”§ â¬œ â¬œ \n",
      "ðŸª™ â¬œ â¬œ â¬œ â¬œ \n",
      "ðŸŽ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 4, Collected: 0\n",
      "\n",
      "ðŸ§  Decided: move (prob-based)\n",
      "âš ï¸ Execution error: action failed\n",
      "ðŸ“ Agent observed: Cannot move invalid from [1, 0] - blocked by boundary (Energy: 0.90)\n",
      "ðŸ¤– Agent action: move east\n",
      "ðŸ“ Agent observed: Moved east from [1, 0] to [2, 0] (Energy: 0.85)\n",
      "ðŸ¤– Agent action: move east\n",
      "ðŸ“ Agent observed: Moved east from [2, 0] to [3, 0] (Energy: 0.80)\n",
      "ðŸ¤– Agent action: search (4, 0)\n",
      "ðŸ“ Agent observed: Searched location (3, 0) (Energy: 0.75)\n",
      "ðŸ¤– Agent action: collect (4, 0)\n",
      "ðŸ“ Agent observed: Nothing to collect at (3, 0) (Energy: 0.70)\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "â¬œ â¬œ â¬œ ðŸ¤– ðŸ—ï¸  \n",
      "â¬œ â¬œ ðŸ”§ â¬œ â¬œ \n",
      "ðŸª™ â¬œ â¬œ â¬œ â¬œ \n",
      "ðŸŽ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 4, Collected: 0\n",
      "\n",
      "ðŸ§  Decided: move (prob-based)\n",
      "ðŸ¤” Making suboptimal plan due to cognitive limitations\n",
      "ðŸ¤– Agent action: move west\n",
      "ðŸ“ Agent observed: Moved west from [3, 0] to [2, 0] (Energy: 0.65)\n",
      "ðŸ¤– Agent action: move west\n",
      "ðŸ“ Agent observed: Moved west from [2, 0] to [1, 0] (Energy: 0.60)\n",
      "ðŸ¤– Agent action: move west\n",
      "ðŸ“ Agent observed: Moved west from [1, 0] to [0, 0] (Energy: 0.55)\n",
      "ðŸ¤– Agent action: move south\n",
      "ðŸ“ Agent observed: Moved south from [0, 0] to [0, 1] (Energy: 0.50)\n",
      "ðŸ”„ Agent changed mind, abandoning current plan\n",
      "ðŸ§  Decided: move (prob-based)\n",
      "ðŸ¤– Agent action: move east\n",
      "ðŸ“ Agent observed: Moved east from [0, 1] to [1, 1] (Energy: 0.45)\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "â¬œ â¬œ â¬œ â¬œ ðŸ—ï¸  \n",
      "â¬œ ðŸ¤– ðŸ”§ â¬œ â¬œ \n",
      "ðŸª™ â¬œ â¬œ â¬œ â¬œ \n",
      "ðŸŽ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 4, Collected: 0\n",
      "\n",
      "ðŸ¤– Agent action: move east\n",
      "ðŸ“ Agent observed: Moved east from [1, 1] to [2, 1] (Energy: 0.40)\n",
      "âš ï¸ Execution error: action failed\n",
      "ðŸ“ Agent observed: Cannot move invalid from [2, 1] - blocked by boundary (Energy: 0.35)\n",
      "ðŸ¤– Agent action: move east\n",
      "ðŸ“ Agent observed: Moved east from [2, 1] to [3, 1] (Energy: 0.30)\n",
      "ðŸ”„ Agent changed mind, abandoning current plan\n",
      "ðŸ§  Decided: move (prob-based)\n",
      "ðŸ¤– Agent action: move west\n",
      "ðŸ“ Agent observed: Moved west from [3, 1] to [2, 1] (Energy: 0.25)\n",
      "ðŸ¤– Agent action: move west\n",
      "ðŸ“ Agent observed: Moved west from [2, 1] to [1, 1] (Energy: 0.20)\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "â¬œ â¬œ â¬œ â¬œ ðŸ—ï¸  \n",
      "â¬œ ðŸ¤– ðŸ”§ â¬œ â¬œ \n",
      "ðŸª™ â¬œ â¬œ â¬œ â¬œ \n",
      "ðŸŽ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 4, Collected: 0\n",
      "\n",
      "ðŸ¤– Agent action: move west\n",
      "ðŸ“ Agent observed: Moved west from [1, 1] to [0, 1] (Energy: 0.15)\n",
      "ðŸ¤– Agent action: move south\n",
      "ðŸ“ Agent observed: Moved south from [0, 1] to [0, 2] (Energy: 0.10)\n",
      "ðŸ¤– Agent action: search (0, 2)\n",
      "ðŸ“ Agent observed: Searched location (0, 2) (Energy: 0.05)\n",
      "ðŸ” Items found: ['coin']\n",
      "ðŸ¤– Agent action: collect (0, 2)\n",
      "ðŸ“ Agent observed: Collected coin at (0, 2) (Energy: 0.00)\n",
      "ðŸ” Items found: ['coin']\n",
      "ðŸ§  Decided: rest (prob-based)\n",
      "ðŸ¤” Making suboptimal plan due to cognitive limitations\n",
      "ðŸ¤– Agent action: move south\n",
      "ðŸ“ Agent observed: Moved south from [0, 2] to [0, 3] (Energy: 0.00)\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "â¬œ â¬œ â¬œ â¬œ ðŸ—ï¸  \n",
      "â¬œ â¬œ ðŸ”§ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "ðŸ¤– â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 3, Collected: 1\n",
      "\n",
      "ðŸ¤– Agent action: search (0, 3)\n",
      "ðŸ“ Agent observed: Searched location (0, 3) (Energy: 0.00)\n",
      "ðŸ” Items found: ['food']\n",
      "ðŸ¤– Agent action: collect (0, 3)\n",
      "ðŸ“ Agent observed: Collected food at (0, 3) (Energy: 0.00)\n",
      "ðŸ” Items found: ['food']\n",
      "ðŸ§  Decided: move (prob-based)\n",
      "ðŸ¤– Agent action: move east\n",
      "ðŸ“ Agent observed: Moved east from [0, 3] to [1, 3] (Energy: 0.00)\n",
      "ðŸ¤– Agent action: move east\n",
      "ðŸ“ Agent observed: Moved east from [1, 3] to [2, 3] (Energy: 0.00)\n",
      "ðŸ¤– Agent action: move north\n",
      "ðŸ“ Agent observed: Moved north from [2, 3] to [2, 2] (Energy: 0.00)\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "â¬œ â¬œ â¬œ â¬œ ðŸ—ï¸  \n",
      "â¬œ â¬œ ðŸ”§ â¬œ â¬œ \n",
      "â¬œ â¬œ ðŸ¤– â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "Items remaining: 2, Collected: 2\n",
      "\n",
      "ðŸ¤– Agent action: move north\n",
      "ðŸ“ Agent observed: Moved north from [2, 2] to [2, 1] (Energy: 0.00)\n",
      "ðŸ¤– Agent action: search (2, 1)\n",
      "ðŸ“ Agent observed: Searched location (2, 1) (Energy: 0.00)\n",
      "ðŸ” Items found: ['tool']\n",
      "ðŸ¤– Agent action: collect (2, 1)\n",
      "ðŸ“ Agent observed: Collected tool at (2, 1) (Energy: 0.00)\n",
      "ðŸ” Items found: ['tool']\n",
      "ðŸ§  Decided: rest (prob-based)\n",
      "ðŸ¤– Agent action: rest recover\n",
      "ðŸ“ Agent observed: Agent rested at [2, 1] (Energy: 0.00)\n",
      "\n",
      "ðŸ“ˆ Final Results:\n",
      "   Items collected: 3\n",
      "   Items remaining: 1\n",
      "\n",
      "ðŸ› ï¸ Custom Environment Creation:\n",
      "You can customize:\n",
      "â€¢ Grid size (default: 5x5)\n",
      "â€¢ Number of items (default: 4)\n",
      "â€¢ Random seed for reproducibility\n",
      "\n",
      "ðŸ“‹ Example Configurations:\n",
      "\n",
      "ðŸ·ï¸ Small & Simple:\n",
      "ðŸŒ Generated 2 items randomly on 3x3 grid\n",
      "ðŸ“¦ Item locations: {(0, 1): 'scroll', (1, 0): 'treasure'}\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "ðŸ¤– ðŸ’° â¬œ \n",
      "ðŸ“œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ \n",
      "Items remaining: 2, Collected: 0\n",
      "\n",
      "\n",
      "ðŸ·ï¸ Large & Crowded:\n",
      "   âš ï¸ Configuration failed: Sample larger than population or is negative\n",
      "\n",
      "ðŸ·ï¸ Needle in Haystack:\n",
      "ðŸŒ Generated 1 items randomly on 5x5 grid\n",
      "ðŸ“¦ Item locations: {(4, 4): 'key'}\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "ðŸ¤– â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ ðŸ—ï¸  \n",
      "Items remaining: 1, Collected: 0\n",
      "\n",
      "\n",
      "ðŸ·ï¸ Item Dense:\n",
      "ðŸŒ Generated 8 items randomly on 4x4 grid\n",
      "ðŸ“¦ Item locations: {(2, 0): 'treasure', (1, 0): 'potion', (0, 3): 'scroll', (1, 2): 'coin', (0, 2): 'key', (2, 2): 'food', (1, 3): 'gem', (1, 1): 'tool'}\n",
      "ðŸ—ºï¸  Environment Map:\n",
      "ðŸ¤– ðŸ§ª ðŸ’° â¬œ \n",
      "â¬œ ðŸ”§ â¬œ â¬œ \n",
      "ðŸ—ï¸  ðŸª™ ðŸŽ â¬œ \n",
      "ðŸ“œ ðŸ’Ž â¬œ â¬œ \n",
      "Items remaining: 8, Collected: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ActionType(Enum):\n",
    "    MOVE = \"move\"\n",
    "    SEARCH = \"search\"\n",
    "    COLLECT = \"collect\"\n",
    "    EXPLORE = \"explore\"\n",
    "    REST = \"rest\"\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "    type: ActionType\n",
    "    target: str\n",
    "    parameters: Dict[str, Any] = None\n",
    "    confidence: float = 1.0  # How confident the agent is in this action\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.parameters is None:\n",
    "            self.parameters = {}\n",
    "\n",
    "@dataclass\n",
    "class Observation:\n",
    "    location: str\n",
    "    items_found: List[str]\n",
    "    energy_level: float\n",
    "    message: str\n",
    "    success: bool = True\n",
    "\n",
    "class Environment:\n",
    "    \"\"\"Grid world environment with random item placement\"\"\"\n",
    "    \n",
    "    def __init__(self, size: int = 5, num_items: int = 4, seed: Optional[int] = None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            \n",
    "        self.size = size\n",
    "        self.agent_pos = [0, 0]  # Agent always starts at (0,0)\n",
    "        self.collected_items = []\n",
    "        \n",
    "        # Generate random items\n",
    "        self.items = self._generate_random_items(num_items)\n",
    "        \n",
    "        print(f\"ðŸŒ Generated {len(self.items)} items randomly on {size}x{size} grid\")\n",
    "        print(f\"ðŸ“¦ Item locations: {dict(self.items)}\")\n",
    "    \n",
    "    def _generate_random_items(self, num_items: int) -> Dict[Tuple[int, int], str]:\n",
    "        \"\"\"Generate random item placements\"\"\"\n",
    "        item_types = [\"treasure\", \"key\", \"food\", \"tool\", \"gem\", \"coin\", \"potion\", \"scroll\"]\n",
    "        \n",
    "        # Get all possible positions except agent start position\n",
    "        all_positions = [(x, y) for x in range(self.size) for y in range(self.size)]\n",
    "        available_positions = [pos for pos in all_positions if pos != (0, 0)]\n",
    "        \n",
    "        # Ensure we don't try to place more items than available positions\n",
    "        num_items = min(num_items, len(available_positions))\n",
    "        \n",
    "        # Randomly select positions\n",
    "        selected_positions = random.sample(available_positions, num_items)\n",
    "        \n",
    "        # Randomly assign item types\n",
    "        selected_items = random.sample(item_types, num_items)\n",
    "        \n",
    "        return dict(zip(selected_positions, selected_items))\n",
    "    \n",
    "    def execute_action(self, action: Action) -> Observation:\n",
    "        \"\"\"Execute an action and return observation\"\"\"\n",
    "        items_here = []  # Initialize items_here for all action types\n",
    "        success = True\n",
    "        \n",
    "        if action.type == ActionType.MOVE:\n",
    "            direction = action.target\n",
    "            old_pos = self.agent_pos.copy()\n",
    "            \n",
    "            if direction == \"north\" and self.agent_pos[1] > 0:\n",
    "                self.agent_pos[1] -= 1\n",
    "            elif direction == \"south\" and self.agent_pos[1] < self.size - 1:\n",
    "                self.agent_pos[1] += 1\n",
    "            elif direction == \"east\" and self.agent_pos[0] < self.size - 1:\n",
    "                self.agent_pos[0] += 1\n",
    "            elif direction == \"west\" and self.agent_pos[0] > 0:\n",
    "                self.agent_pos[0] -= 1\n",
    "            else:\n",
    "                success = False\n",
    "                message = f\"Cannot move {direction} from {old_pos} - blocked by boundary\"\n",
    "            \n",
    "            if success:\n",
    "                message = f\"Moved {direction} from {old_pos} to {self.agent_pos}\"\n",
    "            \n",
    "        elif action.type == ActionType.SEARCH:\n",
    "            current_pos = tuple(self.agent_pos)\n",
    "            if current_pos in self.items:\n",
    "                items_here.append(self.items[current_pos])\n",
    "            message = f\"Searched location {current_pos}\"\n",
    "            \n",
    "        elif action.type == ActionType.COLLECT:\n",
    "            current_pos = tuple(self.agent_pos)\n",
    "            if current_pos in self.items:\n",
    "                item = self.items[current_pos]\n",
    "                self.collected_items.append(item)\n",
    "                del self.items[current_pos]\n",
    "                items_here.append(item)\n",
    "                message = f\"Collected {item} at {current_pos}\"\n",
    "            else:\n",
    "                success = False\n",
    "                message = f\"Nothing to collect at {current_pos}\"\n",
    "                \n",
    "        elif action.type == ActionType.REST:\n",
    "            message = f\"Agent rested at {self.agent_pos}\"\n",
    "            \n",
    "        elif action.type == ActionType.EXPLORE:\n",
    "            # Random exploration move\n",
    "            directions = [\"north\", \"south\", \"east\", \"west\"]\n",
    "            random_direction = random.choice(directions)\n",
    "            old_pos = self.agent_pos.copy()\n",
    "            \n",
    "            if random_direction == \"north\" and self.agent_pos[1] > 0:\n",
    "                self.agent_pos[1] -= 1\n",
    "            elif random_direction == \"south\" and self.agent_pos[1] < self.size - 1:\n",
    "                self.agent_pos[1] += 1\n",
    "            elif random_direction == \"east\" and self.agent_pos[0] < self.size - 1:\n",
    "                self.agent_pos[0] += 1\n",
    "            elif random_direction == \"west\" and self.agent_pos[0] > 0:\n",
    "                self.agent_pos[0] -= 1\n",
    "            else:\n",
    "                success = False\n",
    "            \n",
    "            if success:\n",
    "                message = f\"Explored {random_direction} from {old_pos} to {self.agent_pos}\"\n",
    "            else:\n",
    "                message = f\"Exploration blocked - hit boundary\"\n",
    "                \n",
    "        else:\n",
    "            message = f\"Executed {action.type.value} action\"\n",
    "        \n",
    "        return Observation(\n",
    "            location=f\"{self.agent_pos}\",\n",
    "            items_found=items_here,\n",
    "            energy_level=random.uniform(0.7, 1.0),\n",
    "            message=message,\n",
    "            success=success\n",
    "        )\n",
    "    \n",
    "    def get_state(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current environment state\"\"\"\n",
    "        return {\n",
    "            \"agent_position\": self.agent_pos,\n",
    "            \"remaining_items\": list(self.items.keys()),\n",
    "            \"collected_items\": self.collected_items,\n",
    "            \"total_items\": len(self.items) + len(self.collected_items)\n",
    "        }\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Simple text visualization of the environment\"\"\"\n",
    "        print(\"ðŸ—ºï¸  Environment Map:\")\n",
    "        for y in range(self.size):\n",
    "            row = \"\"\n",
    "            for x in range(self.size):\n",
    "                if [x, y] == self.agent_pos:\n",
    "                    row += \"ðŸ¤– \"\n",
    "                elif (x, y) in self.items:\n",
    "                    # Show different emojis for different items\n",
    "                    item = self.items[(x, y)]\n",
    "                    emoji_map = {\n",
    "                        \"treasure\": \"ðŸ’°\",\n",
    "                        \"key\": \"ðŸ—ï¸ \",\n",
    "                        \"food\": \"ðŸŽ\",\n",
    "                        \"tool\": \"ðŸ”§\",\n",
    "                        \"gem\": \"ðŸ’Ž\",\n",
    "                        \"coin\": \"ðŸª™\",\n",
    "                        \"potion\": \"ðŸ§ª\",\n",
    "                        \"scroll\": \"ðŸ“œ\"\n",
    "                    }\n",
    "                    row += emoji_map.get(item, \"ðŸ“¦\") + \" \"\n",
    "                else:\n",
    "                    row += \"â¬œ \"\n",
    "            print(row)\n",
    "        print(f\"Items remaining: {len(self.items)}, Collected: {len(self.collected_items)}\")\n",
    "        print()\n",
    "\n",
    "class EnhancedAgent:\n",
    "    \"\"\"An AI agent with probabilistic decision-making\"\"\"\n",
    "    \n",
    "    def __init__(self, goal: str = \"collect_all_items\", personality: str = \"balanced\"):\n",
    "        self.goal = goal\n",
    "        self.memory = []\n",
    "        self.knowledge = {}\n",
    "        self.plan = []\n",
    "        self.step_count = 0\n",
    "        self.energy = 1.0\n",
    "        self.personality = personality  # \"cautious\", \"aggressive\", \"balanced\", \"explorer\"\n",
    "        \n",
    "        # Decision-making parameters based on personality\n",
    "        self.decision_params = self._set_personality_params()\n",
    "    \n",
    "    def _set_personality_params(self) -> Dict[str, float]:\n",
    "        \"\"\"Set decision-making parameters based on personality\"\"\"\n",
    "        params = {\n",
    "            \"cautious\": {\n",
    "                \"exploration_rate\": 0.1,\n",
    "                \"risk_tolerance\": 0.2,\n",
    "                \"planning_horizon\": 5,\n",
    "                \"energy_threshold\": 0.6\n",
    "            },\n",
    "            \"aggressive\": {\n",
    "                \"exploration_rate\": 0.05,\n",
    "                \"risk_tolerance\": 0.8,\n",
    "                \"planning_horizon\": 2,\n",
    "                \"energy_threshold\": 0.2\n",
    "            },\n",
    "            \"balanced\": {\n",
    "                \"exploration_rate\": 0.15,\n",
    "                \"risk_tolerance\": 0.5,\n",
    "                \"planning_horizon\": 3,\n",
    "                \"energy_threshold\": 0.4\n",
    "            },\n",
    "            \"explorer\": {\n",
    "                \"exploration_rate\": 0.4,\n",
    "                \"risk_tolerance\": 0.6,\n",
    "                \"planning_horizon\": 1,\n",
    "                \"energy_threshold\": 0.3\n",
    "            }\n",
    "        }\n",
    "        return params.get(self.personality, params[\"balanced\"])\n",
    "    \n",
    "    def perceive(self, observation: Observation) -> None:\n",
    "        \"\"\"Process observations with uncertainty\"\"\"\n",
    "        self.memory.append(observation)\n",
    "        self.energy = max(0, self.energy - 0.05)  # Energy decreases with actions\n",
    "        \n",
    "        # Sometimes miss information due to \"sensor noise\"\n",
    "        if random.random() > 0.1:  # 90% accuracy in perception\n",
    "            if observation.items_found:\n",
    "                loc = observation.location\n",
    "                if loc not in self.knowledge:\n",
    "                    self.knowledge[loc] = []\n",
    "                for item in observation.items_found:\n",
    "                    if item not in self.knowledge[loc]:\n",
    "                        self.knowledge[loc].append(item)\n",
    "        \n",
    "        print(f\"ðŸ“ Agent observed: {observation.message} (Energy: {self.energy:.2f})\")\n",
    "        if observation.items_found:\n",
    "            print(f\"ðŸ” Items found: {observation.items_found}\")\n",
    "    \n",
    "    def evaluate_options(self, env_state: Dict[str, Any]) -> List[Tuple[Action, float]]:\n",
    "        \"\"\"Evaluate multiple action options with scores\"\"\"\n",
    "        remaining_items = env_state[\"remaining_items\"]\n",
    "        agent_pos = env_state[\"agent_position\"]\n",
    "        options = []\n",
    "        \n",
    "        if not remaining_items:\n",
    "            return []\n",
    "        \n",
    "        # Option 1: Go to nearest item (greedy)\n",
    "        nearest_item = min(remaining_items, \n",
    "                         key=lambda pos: abs(pos[0] - agent_pos[0]) + abs(pos[1] - agent_pos[1]))\n",
    "        distance = abs(nearest_item[0] - agent_pos[0]) + abs(nearest_item[1] - agent_pos[1])\n",
    "        greedy_score = 1.0 / (distance + 1)  # Higher score for closer items\n",
    "        options.append((Action(ActionType.MOVE, f\"towards_{nearest_item}\"), greedy_score))\n",
    "        \n",
    "        # Option 2: Go to most valuable item (if we have value estimates)\n",
    "        if len(remaining_items) > 1:\n",
    "            # Simulate item values (in real systems, this might come from experience)\n",
    "            valued_items = [(item, random.uniform(0.3, 1.0)) for item in remaining_items]\n",
    "            best_valued = max(valued_items, key=lambda x: x[1])\n",
    "            value_score = best_valued[1] * 0.8  # Slightly lower than greedy to create tension\n",
    "            options.append((Action(ActionType.MOVE, f\"towards_{best_valued[0]}\"), value_score))\n",
    "        \n",
    "        # Option 3: Explore unknown areas\n",
    "        if random.random() < self.decision_params[\"exploration_rate\"]:\n",
    "            explore_score = 0.3 + random.uniform(0, 0.4)\n",
    "            options.append((Action(ActionType.EXPLORE, \"random_direction\"), explore_score))\n",
    "        \n",
    "        # Option 4: Rest if energy is low\n",
    "        if self.energy < self.decision_params[\"energy_threshold\"]:\n",
    "            rest_score = (self.decision_params[\"energy_threshold\"] - self.energy) * 2\n",
    "            options.append((Action(ActionType.REST, \"recover_energy\"), rest_score))\n",
    "        \n",
    "        return options\n",
    "    \n",
    "    def make_decision(self, options: List[Tuple[Action, float]]) -> Action:\n",
    "        \"\"\"Make probabilistic decision based on option scores\"\"\"\n",
    "        if not options:\n",
    "            return None\n",
    "        \n",
    "        # Add randomization to decision making\n",
    "        actions, scores = zip(*options)\n",
    "        \n",
    "        # Method 1: Weighted random selection (exploration vs exploitation)\n",
    "        if random.random() < self.decision_params[\"exploration_rate\"]:\n",
    "            # Explore: choose randomly\n",
    "            chosen_action = random.choice(actions)\n",
    "            print(f\"ðŸŽ² Exploring: chose {chosen_action.type.value} randomly\")\n",
    "        else:\n",
    "            # Exploit: choose based on scores with some randomness\n",
    "            # Add noise to scores to create uncertainty\n",
    "            noisy_scores = [score + random.uniform(-0.2, 0.2) for score in scores]\n",
    "            \n",
    "            # Softmax-like selection (higher scores more likely, but not guaranteed)\n",
    "            if max(noisy_scores) > 0:\n",
    "                # Normalize scores and add temperature for randomness\n",
    "                temperature = 0.5  # Higher = more random\n",
    "                exp_scores = [pow(max(0, score), 1/temperature) for score in noisy_scores]\n",
    "                total = sum(exp_scores)\n",
    "                \n",
    "                if total > 0:\n",
    "                    probabilities = [score/total for score in exp_scores]\n",
    "                    chosen_action = random.choices(actions, weights=probabilities)[0]\n",
    "                    print(f\"ðŸ§  Decided: {chosen_action.type.value} (prob-based)\")\n",
    "                else:\n",
    "                    chosen_action = random.choice(actions)\n",
    "            else:\n",
    "                chosen_action = random.choice(actions)\n",
    "        \n",
    "        return chosen_action\n",
    "    \n",
    "    def plan_with_uncertainty(self, env_state: Dict[str, Any]) -> List[Action]:\n",
    "        \"\"\"Create plans that account for uncertainty\"\"\"\n",
    "        options = self.evaluate_options(env_state)\n",
    "        \n",
    "        if not options:\n",
    "            return []\n",
    "        \n",
    "        chosen_action = self.make_decision(options)\n",
    "        \n",
    "        # Sometimes make suboptimal plans due to \"bounded rationality\"\n",
    "        if random.random() < 0.15:  # 15% chance of suboptimal planning\n",
    "            print(\"ðŸ¤” Making suboptimal plan due to cognitive limitations\")\n",
    "            remaining_items = env_state[\"remaining_items\"]\n",
    "            if remaining_items:\n",
    "                # Choose a random item instead of optimal\n",
    "                random_item = random.choice(remaining_items)\n",
    "                return self._create_path_to_item(random_item, env_state[\"agent_position\"])\n",
    "        \n",
    "        # Normal planning\n",
    "        if chosen_action and chosen_action.type == ActionType.MOVE:\n",
    "            target_str = chosen_action.target.replace(\"towards_\", \"\")\n",
    "            try:\n",
    "                # Parse the target coordinates\n",
    "                target = eval(target_str)  # In real code, use proper parsing\n",
    "                return self._create_path_to_item(target, env_state[\"agent_position\"])\n",
    "            except:\n",
    "                # Fallback to nearest item\n",
    "                remaining_items = env_state[\"remaining_items\"]\n",
    "                if remaining_items:\n",
    "                    nearest = min(remaining_items, \n",
    "                                key=lambda pos: abs(pos[0] - env_state[\"agent_position\"][0]) + \n",
    "                                               abs(pos[1] - env_state[\"agent_position\"][1]))\n",
    "                    return self._create_path_to_item(nearest, env_state[\"agent_position\"])\n",
    "        \n",
    "        elif chosen_action and chosen_action.type == ActionType.EXPLORE:\n",
    "            # Random exploration\n",
    "            directions = [\"north\", \"south\", \"east\", \"west\"]\n",
    "            random_dir = random.choice(directions)\n",
    "            return [Action(ActionType.MOVE, random_dir)]\n",
    "        \n",
    "        elif chosen_action and chosen_action.type == ActionType.REST:\n",
    "            return [Action(ActionType.REST, \"recover\")]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _create_path_to_item(self, target_item: Tuple[int, int], agent_pos: List[int]) -> List[Action]:\n",
    "        \"\"\"Create path with occasional mistakes\"\"\"\n",
    "        plan = []\n",
    "        target_x, target_y = target_item\n",
    "        current_x, current_y = agent_pos\n",
    "        \n",
    "        # Sometimes choose suboptimal path ordering\n",
    "        if random.random() < 0.3:  # 30% chance to go vertical first instead\n",
    "            # Move vertically first\n",
    "            while current_y != target_y:\n",
    "                if current_y < target_y:\n",
    "                    plan.append(Action(ActionType.MOVE, \"south\"))\n",
    "                    current_y += 1\n",
    "                else:\n",
    "                    plan.append(Action(ActionType.MOVE, \"north\"))\n",
    "                    current_y -= 1\n",
    "            \n",
    "            # Then horizontally\n",
    "            while current_x != target_x:\n",
    "                if current_x < target_x:\n",
    "                    plan.append(Action(ActionType.MOVE, \"east\"))\n",
    "                    current_x += 1\n",
    "                else:\n",
    "                    plan.append(Action(ActionType.MOVE, \"west\"))\n",
    "                    current_x -= 1\n",
    "        else:\n",
    "            # Standard horizontal-first approach\n",
    "            while current_x != target_x:\n",
    "                if current_x < target_x:\n",
    "                    plan.append(Action(ActionType.MOVE, \"east\"))\n",
    "                    current_x += 1\n",
    "                else:\n",
    "                    plan.append(Action(ActionType.MOVE, \"west\"))\n",
    "                    current_x -= 1\n",
    "            \n",
    "            while current_y != target_y:\n",
    "                if current_y < target_y:\n",
    "                    plan.append(Action(ActionType.MOVE, \"south\"))\n",
    "                    current_y += 1\n",
    "                else:\n",
    "                    plan.append(Action(ActionType.MOVE, \"north\"))\n",
    "                    current_y -= 1\n",
    "        \n",
    "        # Add search and collect actions\n",
    "        plan.append(Action(ActionType.SEARCH, str(target_item)))\n",
    "        plan.append(Action(ActionType.COLLECT, str(target_item)))\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def act_with_uncertainty(self, environment) -> Optional[Action]:\n",
    "        \"\"\"Act with probabilistic decision making\"\"\"\n",
    "        \n",
    "        # Sometimes forget the current plan (represents distraction/changing mind)\n",
    "        if self.plan and random.random() < 0.1:  # 10% chance to abandon current plan\n",
    "            print(\"ðŸ”„ Agent changed mind, abandoning current plan\")\n",
    "            self.plan = []\n",
    "        \n",
    "        # If no plan, create new one\n",
    "        if not self.plan:\n",
    "            env_state = environment.get_state()\n",
    "            self.plan = self.plan_with_uncertainty(env_state)\n",
    "            \n",
    "            if not self.plan:\n",
    "                print(\"ðŸŽ¯ Goal achieved! No more actions needed.\")\n",
    "                return None\n",
    "        \n",
    "        # Execute next action with possible errors\n",
    "        if self.plan:\n",
    "            action = self.plan.pop(0)\n",
    "            \n",
    "            # Sometimes make execution errors\n",
    "            if random.random() < 0.05:  # 5% chance of execution error\n",
    "                print(\"âš ï¸ Execution error: action failed\")\n",
    "                return Action(ActionType.MOVE, \"invalid\")  # This will fail gracefully\n",
    "            \n",
    "            print(f\"ðŸ¤– Agent action: {action.type.value} {action.target}\")\n",
    "            return action\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Demo function to show random environments\n",
    "def demo_random_environments():\n",
    "    \"\"\"Demonstrate multiple random environment generations\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ² Random Environment Generation Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Generate 3 different random environments\n",
    "    for i in range(3):\n",
    "        print(f\"\\nðŸŒ Random Environment #{i+1}:\")\n",
    "        env = Environment(size=6, num_items=5)\n",
    "        env.visualize()\n",
    "        \n",
    "        # Show some stats\n",
    "        positions = list(env.items.keys())\n",
    "        distances = [abs(pos[0]) + abs(pos[1]) for pos in positions]\n",
    "        avg_distance = sum(distances) / len(distances) if distances else 0\n",
    "        \n",
    "        print(f\"ðŸ“Š Average distance from start: {avg_distance:.1f}\")\n",
    "        print(f\"ðŸ“¦ Item types: {list(env.items.values())}\")\n",
    "    \n",
    "    print(\"\\nðŸ”¬ Comparing Agent Performance on Random vs Fixed Environments:\")\n",
    "    \n",
    "    # Test agent on random environment\n",
    "    print(\"\\nðŸŽ¯ Agent on Random Environment:\")\n",
    "    random_env = Environment(size=5, num_items=4)\n",
    "    random_env.visualize()\n",
    "    \n",
    "    agent = EnhancedAgent(personality=\"balanced\")\n",
    "    steps_taken = 0\n",
    "    max_steps = 30\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = agent.act_with_uncertainty(random_env)\n",
    "        if action is None:\n",
    "            break\n",
    "            \n",
    "        observation = random_env.execute_action(action)\n",
    "        agent.perceive(observation)\n",
    "        steps_taken += 1\n",
    "        \n",
    "        # Show progress every few steps\n",
    "        if step % 5 == 0 or not random_env.items:\n",
    "            random_env.visualize()\n",
    "            \n",
    "        if not random_env.items:\n",
    "            print(f\"ðŸŽ‰ Success! Completed in {steps_taken} steps\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    final_state = random_env.get_state()\n",
    "    print(f\"\\nðŸ“ˆ Final Results:\")\n",
    "    print(f\"   Items collected: {len(final_state['collected_items'])}\")\n",
    "    print(f\"   Items remaining: {len(final_state['remaining_items'])}\")\n",
    "\n",
    "def create_custom_environment():\n",
    "    \"\"\"Create environment with custom parameters\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ› ï¸ Custom Environment Creation:\")\n",
    "    print(\"You can customize:\")\n",
    "    print(\"â€¢ Grid size (default: 5x5)\")\n",
    "    print(\"â€¢ Number of items (default: 4)\")\n",
    "    print(\"â€¢ Random seed for reproducibility\")\n",
    "    \n",
    "    # Example custom environments\n",
    "    print(\"\\nðŸ“‹ Example Configurations:\")\n",
    "    \n",
    "    configs = [\n",
    "        {\"size\": 3, \"num_items\": 2, \"name\": \"Small & Simple\"},\n",
    "        {\"size\": 8, \"num_items\": 10, \"name\": \"Large & Crowded\"},\n",
    "        {\"size\": 5, \"num_items\": 1, \"name\": \"Needle in Haystack\"},\n",
    "        {\"size\": 4, \"num_items\": 8, \"name\": \"Item Dense\"}\n",
    "    ]\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\nðŸ·ï¸ {config['name']}:\")\n",
    "        try:\n",
    "            env = Environment(size=config['size'], num_items=config['num_items'])\n",
    "            env.visualize()\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Configuration failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_random_environments()\n",
    "    create_custom_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730e229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
